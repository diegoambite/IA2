{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica 2. Procesamiento de lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grupo 16: Adina Han y Diego Ambite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 1. Análisis de sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"yelp_labelled.txt\",\"r\") as text_file:\n",
    "    lines_org = text_file.read().split('\\n')\n",
    "len(lines_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [line.split(\"\\t\") for line in lines_org if len(line.split(\"\\t\"))==2 and line.split(\"\\t\")[1]!='']\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_line = np.array(lines)\n",
    "text = data_line[:,0]\n",
    "target = data_line[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Texto':text,'Target':target}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = df.Target.map({'0':'Mala','1':'Buena'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message_len'] = df.Texto.apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "df[df.Label=='Mala'].message_len.plot(bins=35, kind='hist', color='blue', \n",
    "                                       label='Bad messages', alpha=0.6)\n",
    "df[df.Label=='Buena'].message_len.plot(kind='hist', color='red', \n",
    "                                       label='Good messages', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Message Length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Label=='Mala'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Label=='Buena'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apartado a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configura una partición train-test usando el 75% de los datos para entrenamiento y el 25% restante para test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Texto'], df['Target'], test_size=0.25, random_state=333)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos a estudiar varias representaciones de bolsa de palabras, pero todas ellas utilizarán countVectorizer con el diccionario que se crea a partir de los términos del propio corpus y la lista de palabras vacías (stop_words) que proporciona sklearn para el inglés. Las 4 posibilidades que estudiaremos surgen de combinar los siguientes 2 parámetros:\n",
    "\n",
    "    - Bolsa de palabras binaria (usando el countVectorizer con el parámetro binary=True y sin usar TfidfTransformer) y bolsa de palabras con TF/IDF (usando primero el countVectorizer con el parámetro binary=False, y sobre el resultado el TfidfTransformer)\n",
    "    \n",
    "    - Usando un rango de n-gramas de (1,1) y de (1,2) (parámetro ngram_range del countVectorizer). Es decir, haciendo que la bolsa de palabras se consideren solamente monogramas, o que se consideren monogramas y bigramas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_binary_ngram11 = CountVectorizer(stop_words='english',binary=True, ngram_range=(1,1))\n",
    "# Tomamos los textos del conjunto de entrenamiento y los transformamos en \n",
    "# una matriz de datos (palabras) según el diccionario estándar\n",
    "X_train_binary_ngram11=vectorizer_binary_ngram11.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_binary_ngram11 = vectorizer_binary_ngram11.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_binary_ngram11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_binary_ngram11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_binary_ngram11.vocabulary_\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer_binary_ngram11.get_feature_names()\n",
    "\n",
    "print(len(feature_names))\n",
    "print(feature_names)\n",
    "#print(train_vector_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf_ngram11 = CountVectorizer(stop_words='english',binary=False, ngram_range=(1,1))\n",
    "X_train_tfidf_ngram11=vectorizer_tfidf_ngram11.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf_ngram11=vectorizer_tfidf_ngram11.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf_ngram11.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_tfidf = vectorizer_tfidf_ngram11.get_feature_names()\n",
    "\n",
    "print(len(feature_names_tfidf))\n",
    "print(feature_names_tfidf)\n",
    "#print(train_vector_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el valor TF-IDF \n",
    "tfidfer = TfidfTransformer()\n",
    "train_tfidf_preprocessed = tfidfer.fit_transform(X_train_tfidf_ngram11)\n",
    "test_tfidf_preprocessed = tfidfer.transform(X_test_tfidf_ngram11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_binary_ngrams12 = CountVectorizer(stop_words='english',binary=True, ngram_range=(1,2))\n",
    "X_train_binary_ngrams12 = vectorizer_binary_ngrams12.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test__binary_ngrams12 = vectorizer_ngrams12.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_ngrams12 = vectorizer_binary_ngrams12.get_feature_names()\n",
    "\n",
    "print(len(feature_names_ngrams12))\n",
    "print(feature_names_ngrams12)\n",
    "#print(train_vector_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf_ngrams12 = CountVectorizer(stop_words='english',binary=False, ngram_range=(1,2))\n",
    "X_train_tfidf_ngrams12 = vectorizer_tfidf_ngrams12.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ifidf_ngrams12 = vectorizer_tfidf_ngrams12.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfer = TfidfTransformer()\n",
    "train_tfidf_preprocessed = tfidfer.fit_transform(X_train_tfidf_ngrams12)\n",
    "test_tfidf_preprocessed = tfidfer.transform(X_test_ifidf_ngrams12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "def write_terms (feature_names, data, vector_data, index):\n",
    "    '''\n",
    "    Escribe los términos presentes en un mensaje representado como bolsa de palabras.\n",
    "    \n",
    "    - feature_names: terminos usados para vectorizar\n",
    "    - data: lista de mensajes original (si data==None no se muestra el mensaje original)\n",
    "    - vector_data: matriz (dispersa) de mensaje vectorizados\n",
    "    - index: posición del mensaje a mostrar\n",
    "    '''\n",
    "    # máscara para seleccionar sólo el mensaje en posición index\n",
    "    mask=vector_data[index,:]>0\n",
    "    \n",
    "    # términos que aparecen en ese mensaje vectorizado\n",
    "    terminos = ma.array(feature_names, mask = ~(mask[0].toarray()))\n",
    "    \n",
    "    # mostrar mensaje original\n",
    "    if data:\n",
    "        print('Mensaje', index, ':', data[index])\n",
    "    \n",
    "    # mostrar términos que aparecen en el mensaje vectorizado\n",
    "    print('Mensaje', index, 'vectorizado:', terminos.compressed(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_terms(feature_names, train_data.data, train_vector_data, 10)\n",
    "write_terms(feature_names, None, X_train_binary, 0)\n",
    "write_terms(feature_names, None, X_train_tfidf, 0)\n",
    "write_terms(feature_names, None, X_train_ngrams11, 0)\n",
    "\n",
    "write_terms(feature_names, None, X_train_binary, 10)\n",
    "write_terms(feature_names, None, X_train_tfidf, 10)\n",
    "write_terms(feature_names, None, X_train_ngrams11, 10)\n",
    "\n",
    "write_terms(feature_names, None, X_train_binary, 100)\n",
    "write_terms(feature_names, None, X_train_tfidf, 100)\n",
    "write_terms(feature_names, None, X_train_ngrams11, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    \n",
    "#### Para cada una de esas 4 combinaciones entrenaremos dos clasificadores:\n",
    "\n",
    "     1. Un clasificador naive bayes, eligiendo el más adecuado para cada caso.\n",
    "     \n",
    "     2. Un árbol de decisión buscando un valor óptimo para uno de los siguientes parámetros para que se maximice la tasa de aciertos en el conjunto de test: max_depth, min_samples_leaf o max_leaf_nodes (siempre el mismo).\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "\n",
    "mnb_classifier.fit(X_train_binary, y_train)\n",
    "\n",
    "mnb_train_predictions = mnb_classifier.predict(X_train_binary)\n",
    "mnb_test_predictions = mnb_classifier.predict(X_test_binary)\n",
    "\n",
    "print(\"Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento:\", np.mean(mnb_train_predictions == y_train))\n",
    "print(\"Multinomial Naive Bayes, porcentaje de aciertos en test:\", np.mean(mnb_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "\n",
    "mnb_classifier.fit(train_tfidf_preprocessed, y_train)\n",
    "\n",
    "mnb_train_predictions = mnb_classifier.predict(train_tfidf_preprocessed)\n",
    "mnb_test_predictions = mnb_classifier.predict(test_tfidf_preprocessed)\n",
    "\n",
    "print(\"Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento:\", np.mean(mnb_train_predictions == y_train))\n",
    "print(\"Multinomial Naive Bayes, porcentaje de aciertos en test:\", np.mean(mnb_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "\n",
    "mnb_classifier.fit(train_ngrams11_preprocessed, y_train)\n",
    "\n",
    "mnb_train_predictions = mnb_classifier.predict(train_ngrams11_preprocessed)\n",
    "mnb_test_predictions = mnb_classifier.predict(test_ngrams11_preprocessed)\n",
    "\n",
    "print(\"Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento:\", np.mean(mnb_train_predictions == y_train))\n",
    "print(\"Multinomial Naive Bayes, porcentaje de aciertos en test:\", np.mean(mnb_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "\n",
    "mnb_classifier.fit(X_train_ngrams11, y_train)\n",
    "\n",
    "mnb_train_predictions = mnb_classifier.predict(X_train_ngrams11)\n",
    "mnb_test_predictions = mnb_classifier.predict(X_test_ngrams11)\n",
    "\n",
    "print(\"Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento:\", np.mean(mnb_train_predictions == y_train))\n",
    "print(\"Multinomial Naive Bayes, porcentaje de aciertos en test:\", np.mean(mnb_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "\n",
    "mnb_classifier.fit(X_train_ngrams12, y_train)\n",
    "\n",
    "mnb_train_predictions = mnb_classifier.predict(X_train_ngrams12)\n",
    "mnb_test_predictions = mnb_classifier.predict(X_test_ngrams12)\n",
    "\n",
    "print(\"Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento:\", np.mean(mnb_train_predictions == y_train))\n",
    "print(\"Multinomial Naive Bayes, porcentaje de aciertos en test:\", np.mean(mnb_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "\n",
    "mnb_classifier.fit(train_ngrams12_preprocessed, y_train)\n",
    "\n",
    "mnb_train_predictions = mnb_classifier.predict(train_ngrams12_preprocessed)\n",
    "mnb_test_predictions = mnb_classifier.predict(test_ngrams12_preprocessed)\n",
    "\n",
    "print(\"Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento:\", np.mean(mnb_train_predictions == y_train))\n",
    "print(\"Multinomial Naive Bayes, porcentaje de aciertos en test:\", np.mean(mnb_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿¿¿¿¿¿¿¿¿¿¿¿¿¿hay que usar tfidftransform() con las n-grams??????????????? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "\n",
    "# Creamos el clasificador con los valores por defecto\n",
    "tree_classifier = tree.DecisionTreeClassifier()\n",
    "tree_classifier.fit(X_train_binary, y_train)\n",
    "\n",
    "tree_train_predictions = tree_classifier.predict(X_train_binary)\n",
    "tree_test_predictions = tree_classifier.predict(X_test_binary)\n",
    "\n",
    "print(\"Árbol, porcentaje de aciertos en entrenamiento:\", np.mean(tree_train_predictions == y_train))\n",
    "print(\"Árbol, porcentaje de aciertos en test:\", np.mean(tree_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import numpy as np\n",
    "\n",
    "# Creamos el clasificador con los valores por defecto\n",
    "tree_classifier = tree.DecisionTreeClassifier()\n",
    "tree_classifier.fit(train_tfidf_preprocessed, y_train)\n",
    "\n",
    "tree_train_predictions = tree_classifier.predict(train_tfidf_preprocessed)\n",
    "tree_test_predictions = tree_classifier.predict(test_tfidf_preprocessed)\n",
    "\n",
    "print(\"Árbol, porcentaje de aciertos en entrenamiento:\", np.mean(tree_train_predictions == y_train))\n",
    "print(\"Árbol, porcentaje de aciertos en test:\", np.mean(tree_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import numpy as np\n",
    "\n",
    "# Creamos el clasificador con los valores por defecto\n",
    "tree_classifier = tree.DecisionTreeClassifier()\n",
    "tree_classifier.fit(train_ngrams11_preprocessed, y_train)\n",
    "\n",
    "tree_train_predictions = tree_classifier.predict(train_ngrams11_preprocessed)\n",
    "tree_test_predictions = tree_classifier.predict(test_ngrams11_preprocessed)\n",
    "\n",
    "print(\"Árbol, porcentaje de aciertos en entrenamiento:\", np.mean(tree_train_predictions == y_train))\n",
    "print(\"Árbol, porcentaje de aciertos en test:\", np.mean(tree_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import numpy as np\n",
    "\n",
    "# Creamos el clasificador con los valores por defecto\n",
    "tree_classifier = tree.DecisionTreeClassifier()\n",
    "tree_classifier.fit(X_train_ngrams11, y_train)\n",
    "\n",
    "tree_train_predictions = tree_classifier.predict(X_train_ngrams11)\n",
    "tree_test_predictions = tree_classifier.predict(X_test_ngrams11)\n",
    "\n",
    "print(\"Árbol, porcentaje de aciertos en entrenamiento:\", np.mean(tree_train_predictions == y_train))\n",
    "print(\"Árbol, porcentaje de aciertos en test:\", np.mean(tree_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import numpy as np\n",
    "\n",
    "# Creamos el clasificador con los valores por defecto\n",
    "tree_classifier = tree.DecisionTreeClassifier()\n",
    "tree_classifier.fit(X_train_ngrams12, y_train)\n",
    "\n",
    "tree_train_predictions = tree_classifier.predict(X_train_ngrams12)\n",
    "tree_test_predictions = tree_classifier.predict(X_test_ngrams12)\n",
    "\n",
    "print(\"Árbol, porcentaje de aciertos en entrenamiento:\", np.mean(tree_train_predictions == y_train))\n",
    "print(\"Árbol, porcentaje de aciertos en test:\", np.mean(tree_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import numpy as np\n",
    "\n",
    "# Creamos el clasificador con los valores por defecto\n",
    "tree_classifier = tree.DecisionTreeClassifier()\n",
    "tree_classifier.fit(train_ngrams12_preprocessed, y_train)\n",
    "\n",
    "tree_train_predictions = tree_classifier.predict(train_ngrams12_preprocessed)\n",
    "tree_test_predictions = tree_classifier.predict(test_ngrams12_preprocessed)\n",
    "\n",
    "print(\"Árbol, porcentaje de aciertos en entrenamiento:\", np.mean(tree_train_predictions == y_train))\n",
    "print(\"Árbol, porcentaje de aciertos en test:\", np.mean(tree_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿¿¿¿¿¿¿¿¿¿es normal que en los arboles tengamos el mismo porcentaje de aciertos de entrenamiento??????????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿¿¿¿¿de los arboles de arriba tenemos que elegir el que tenga mejor porcentaje de aciertos en test????????????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿que valor elegir para max_dept, hasta que punto podemos incrementar este valor ????????????????????????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analiza la tasa de aciertos de entrenamiento y test de los 2 clasificadores en las 4 representaciones de bolsa de palabras (8 configuraciones en total) y contesta a las siguientes preguntas:\n",
    "\n",
    "    - ¿Hay un clasificador que sea superior al otro? ¿por qué crees que sucede?\n",
    "    \n",
    "    - Para cada clasificador, ¿tiene un efecto positivo el añadir “complejidad” a la vectorización? Es decir, añadir bigramas y añadir tf-idf. ¿Por qué crees que sucede este efecto positivo o la falta del mismo? \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados que obtenemos con el arbol de decisión son mejores que los obtenidos con el clasificador de naive bayes. MIRAR ARBOLES DE DECISION Y NAIVE BAYES (para contestar por que creemos que sucede eso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "max_depths = range(1, 40)\n",
    "for md in max_depths: \n",
    "    # Repetimos el proceso pero modificando los parámetros de aprendizaje del árbol\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\",  # por defecto Gini pero podemos cambiar a entropía\n",
    "                                 max_depth=md,          # profundidad máxima del árbol\n",
    "                                 min_samples_split=5,  # mínimo de muestras en el nodo para seguir dividiéndolo\n",
    "                                 random_state=333)\n",
    "    clf = clf.fit(train_ngrams12_preprocessed, y_train)\n",
    "        \n",
    "    #train_accuracy.append(np.mean(scores['train_score']))\n",
    "    #test_accuracy.append(np.mean(scores['test_score']))\n",
    "    # Calculamos la precisión del modelo de entrenamiento y de test\n",
    "    train_accuracy.append(clf.score(train_ngrams12_preprocessed, y_train))\n",
    "    test_accuracy.append(clf.score(test_ngrams12_preprocessed, y_test))\n",
    "    plt.figure(figsize=(50,50))\n",
    "    plot_tree(clf, filled=True, feature_names=feature_names_ngrams12, class_names=df['Label'], rounded=True)\n",
    "    plt.show()\n",
    "train_accuracy, test_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Draw lines\n",
    "plt.plot(max_depths, train_accuracy, color=\"r\",  label=\"Training\")\n",
    "plt.plot(max_depths, test_accuracy, color=\"g\", label=\"Test\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Curva de aprendizaje\")\n",
    "plt.xlabel(\"Parametro\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "min_samples_splits = range(2, 10)\n",
    "for mss in min_samples_splits: \n",
    "    # Repetimos el proceso pero modificando los parámetros de aprendizaje del árbol\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\",  # por defecto Gini pero podemos cambiar a entropía\n",
    "                                 max_depth=5,          # profundidad máxima del árbol\n",
    "                                 min_samples_split=mss,  # mínimo de muestras en el nodo para seguir dividiéndolo\n",
    "                                 random_state=333)\n",
    "    clf = clf.fit(train_ngrams12_preprocessed, y_train)\n",
    "        \n",
    "    #train_accuracy.append(np.mean(scores['train_score']))\n",
    "    #test_accuracy.append(np.mean(scores['test_score']))\n",
    "    # Calculamos la precisión del modelo de entrenamiento y de test\n",
    "    train_accuracy.append(clf.score(train_ngrams12_preprocessed, y_train))\n",
    "    test_accuracy.append(clf.score(test_ngrams12_preprocessed, y_test))\n",
    "    plt.figure(figsize=(50,50))\n",
    "    plot_tree(clf, filled=True, feature_names=feature_names_ngrams12, class_names=df['Label'], rounded=True)\n",
    "    plt.show()\n",
    "train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vamos a mostrar el árbol de decisión generado usando plot_tree\n",
    "plt.figure(figsize=(50,50))\n",
    "plot_tree(clf, filled=True, feature_names=feature_names_ngrams12, class_names=df['Label'], rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Draw lines\n",
    "plt.plot(min_samples_splits, train_accuracy, color=\"r\",  label=\"Training\")\n",
    "plt.plot(min_samples_splits, test_accuracy, color=\"g\", label=\"Test\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Curva de aprendizaje\")\n",
    "plt.xlabel(\"Parametro\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "min_samples_leafs = range(3, 15)\n",
    "for msl in min_samples_leafs: \n",
    "    # Repetimos el proceso pero modificando los parámetros de aprendizaje del árbol\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\",  # por defecto Gini pero podemos cambiar a entropía\n",
    "                                 max_depth=5,          # profundidad máxima del árbol\n",
    "                                 min_samples_split=4,  # mínimo de muestras en el nodo para seguir dividiéndolo\n",
    "                                 min_samples_leaf=msl,\n",
    "                                 random_state=333)\n",
    "    clf = clf.fit(train_ngrams12_preprocessed, y_train)\n",
    "        \n",
    "    #train_accuracy.append(np.mean(scores['train_score']))\n",
    "    #test_accuracy.append(np.mean(scores['test_score']))\n",
    "    # Calculamos la precisión del modelo de entrenamiento y de test\n",
    "    train_accuracy.append(clf.score(train_ngrams12_preprocessed, y_train))\n",
    "    test_accuracy.append(clf.score(test_ngrams12_preprocessed, y_test))\n",
    "    plt.figure(figsize=(50,50))\n",
    "    plot_tree(clf, filled=True, feature_names=feature_names_ngrams12, class_names=df['Label'], rounded=True)\n",
    "    plt.show()\n",
    "train_accuracy, test_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Draw lines\n",
    "plt.plot(min_samples_leafs, train_accuracy, color=\"r\",  label=\"Training\")\n",
    "plt.plot(min_samples_leafs, test_accuracy, color=\"g\", label=\"Test\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Curva de aprendizaje\")\n",
    "plt.xlabel(\"Parametro\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pintando árboles con sckit-learn\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vamos a mostrar el árbol de decisión generado usando plot_tree\n",
    "plt.figure(figsize=(50,50))\n",
    "plot_tree(clf, filled=True, feature_names=feature_names_ngrams12, class_names=df['Label'], rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", \n",
    "                              max_depth=14, \n",
    "                              min_samples_split=4,\n",
    "                              min_samples_leaf=4,\n",
    "                              random_state=333)\n",
    "\n",
    "clf = clf.fit(train_ngrams12_preprocessed, y_train)\n",
    "    \n",
    "train_accuracy = clf.score(train_ngrams12_preprocessed, y_train)\n",
    "test_accuracy = clf.score(test_ngrams12_preprocessed, y_test)\n",
    "train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecciona el mejor árbol de decisión y obtén las 25 variables con más poder discriminante:\n",
    "    \n",
    "    - ¿Predominan más las palabras de uno u otro sentimiento? ¿por qué? ¿hay ruido? \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top20_features_in_trees(vectorizer, clf):\n",
    "    \"\"\"Prints features with the highest coefficient values\"\"\"\n",
    "    feature_names = vectorizer_ngrams12.get_feature_names()\n",
    "    \n",
    "    top20 = np.argsort(clf.feature_importances_)[-20:]\n",
    "    reversed_top = top20[::-1]\n",
    "    print(\"Top 20 features in the tree\\n\")\n",
    "    print(\"%s\" % ( \" / \".join(feature_names[j] for j in reversed_top)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top20_features_in_trees(vectorizer_ngrams12,tree_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecciona el mejor clasificador naive bayes y obtén las 25 variables con más presencia en cada clase:\n",
    "    \n",
    "    - ¿Tienen sentido las palabras seleccionadas? ¿hay ruido (palabras sin sentimiento o de sentimiento opuesto al esperado)? ¿por qué crees que suceden estos fenómenos?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "\n",
    "mnb_classifier.fit(X_train_ngrams12, y_train)\n",
    "\n",
    "mnb_train_predictions = mnb_classifier.predict(X_train_ngrams12)\n",
    "mnb_test_predictions = mnb_classifier.predict(X_test_ngrams12)\n",
    "\n",
    "print(\"Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento:\", np.mean(mnb_train_predictions == y_train))\n",
    "print(\"Multinomial Naive Bayes, porcentaje de aciertos en test:\", np.mean(mnb_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# NOTA: Aquí elegimos analizar un determinado clasificador y sus predicciones \n",
    "# Por ejemplo el naive bayes\n",
    "\n",
    "classifier=mnb_classifier\n",
    "predictions = mnb_test_predictions\n",
    "target_names={'Bad','Good'}\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top25_features_per_class_in_NB(vectorizer, clf, class_labels):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer_ngrams12.get_feature_names()\n",
    "    print(\"Top 25 features per class\\n\")\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top25 = np.argsort(clf.feature_log_prob_[i])[-25:]\n",
    "        reversed_top = top25[::-1]\n",
    "        \n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \" / \".join(feature_names[j] for j in reversed_top)),'\\n')\n",
    "        \n",
    "        #Descomentar para ver el índice de los términos en el diccionario\n",
    "        #print(\"%s \" % (\" / \".join(str(j) for j in reversed_top)),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top25_features_per_class_in_NB(vectorizer_ngrams12,mnb_classifier,target_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalmente, explica de manera razonada las conclusiones que has extraído de todo el estudio realizado en este apartado.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apartado b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toma el mejor clasificador Naive Bayes y el mejor árbol de decisión y analiza a fondo sus resultados en el conjunto de test.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Analiza la precisión y la exhaustividad de cada clasificador en cada una de las clases (opiniones positivas y negativas).\n",
    "    \n",
    "         Para cada clasificador, ¿tiene un comportamiento homogéneo a la hora de clasificar ambas clases?\n",
    "         ¿Cuáles son las fortalezas y debilidades de cada uno de los clasificadores?\n",
    "         ¿Hay algún clasificador que sea mejor que el otro en todo?\n",
    "         ¿Coinciden ambos clasificadores a la hora de clasificar mejor una clase que la otra?\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Pinta los 8 primeros niveles del árbol de decisión y comenta lo que ves.\n",
    "         ¿Qué estructura tiene el árbol?\n",
    "         ¿Cómo interpretas los niveles que has pintado? ¿tienen algún sentido con respecto a la tasa de aciertos, o la precisión y exhaustividad del clasificador? o ¿Hay nodos impuros?\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Por cada clasificador identifica 2 críticas que hayan sido falsas positivas (malas críticas calificadas como buenas) y 2 críticas que han sido falsas negativas (buenas críticas clasificadas como malas). Analiza tanto su texto original, como el vector de palabras resultante (solamente los términos activos).\n",
    "         ¿Por qué crees que ha fallado el clasificador en cada uno de los casos?\n",
    "         ¿Se te ocurre alguna idea sobre cómo mejorar el clasificador de sentimiento?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

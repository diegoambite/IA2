{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica 2. Procesamiento de lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 1. Análisis de sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"yelp_labelled.txt\",\"r\") as text_file:\n",
    "    lines_org = text_file.read().split('\\n')\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [line.split(\"\\t\") for line in lines_org if len(line.split(\"\\t\"))==2 and line.split(\"\\t\")[1]!='']\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apartado a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configura una partición train-test usando el 75% de los datos para entrenamiento y el 25% restante para test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [line[0] for line in lines ]\n",
    "labels = [int(line[1]) for line in lines]\n",
    "X_train = data[:750]\n",
    "X_test = data[750:]\n",
    "y_train = labels[:750]\n",
    "y_test = labels[750:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos a estudiar varias representaciones de bolsa de palabras, pero todas ellas utilizarán countVectorizer con el diccionario que se crea a partir de los términos del propio corpus y la lista de palabras vacías (stop_words) que proporciona sklearn para el inglés. Las 4 posibilidades que estudiaremos surgen de combinar los siguientes 2 parámetros:\n",
    "\n",
    "    - Bolsa de palabras binaria y bolsa de palabras con TF/IDF (parámetro binary).\n",
    "    \n",
    "    - Usando un rango de n-gramas de (1,1) y de (1,2) (parámetro ngram_range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "# Tomamos los textos del conjunto de entrenamiento y los transformamos en \n",
    "# una matriz de datos (palabras) según el diccionario estándar\n",
    "train_vector_data=vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1494\n",
      "['00', '10', '100', '12', '15', '17', '1979', '20', '2007', '23', '30', '35', '40', '40min', '45', '4ths', '5lb', '70', '90', 'absolute', 'absolutely', 'absolutley', 'accident', 'accommodations', 'accomodate', 'accordingly', 'accountant', 'ache', 'acknowledged', 'actual', 'actually', 'added', 'afternoon', 'ago', 'ahead', 'airline', 'airport', 'almonds', 'amazing', 'ambiance', 'ambience', 'ample', 'andddd', 'angry', 'annoying', 'anytime', 'anyways', 'apart', 'apology', 'app', 'appalling', 'apparently', 'appetizer', 'appetizers', 'apple', 'area', 'aria', 'array', 'arrived', 'arriving', 'ask', 'asked', 'asking', 'assure', 'ate', 'atmosphere', 'atrocious', 'attached', 'attack', 'attention', 'attentive', 'attitudes', 'auju', 'authentic', 'average', 'avocado', 'avoid', 'away', 'awesome', 'awkward', 'awkwardly', 'ayce', 'az', 'baba', 'baby', 'bachi', 'bacon', 'bad', 'bagels', 'bakery', 'baklava', 'ball', 'bamboo', 'banana', 'bank', 'bar', 'barely', 'bargain', 'bars', 'bartender', 'baseball', 'based', 'basically', 'bathroom', 'bathrooms', 'batter', 'bay', 'bbq', 'bean', 'beans', 'beat', 'beateous', 'beautiful', 'beautifully', 'beauty', 'beef', 'beer', 'begin', 'believe', 'bellies', 'belly', 'best', 'better', 'big', 'bird', 'biscuits', 'bisque', 'bit', 'bitches', 'bite', 'bites', 'bits', 'black', 'blah', 'bland', 'blandest', 'blanket', 'bloddy', 'bloody', 'blow', 'blows', 'blue', 'boba', 'bodes', 'boiled', 'bone', 'book', 'boot', 'boring', 'bouchon', 'bowl', 'box', 'boy', 'boyfriend', 'boys', 'bread', 'break', 'breakfast', 'breaks', 'breeze', 'brick', 'bring', 'brings', 'brought', 'brunch', 'bucks', 'buffet', 'buffets', 'bug', 'building', 'buldogis', 'bunch', 'burger', 'burgers', 'burned', 'burrittos', 'bus', 'business', 'businesses', 'bussell', 'busy', 'butter', 'buying', 'bye', 'caesar', 'cafe', 'café', 'cake', 'cakes', 'calligraphy', 'callings', 'came', 'camelback', 'cannoli', 'cape', 'capers', 'car', 'care', 'carly', 'carpaccio', 'cartel', 'cash', 'cashier', 'casino', 'caught', 'cavier', 'certainly', 'changing', 'char', 'charcoal', 'charged', 'charming', 'cheap', 'cheated', 'check', 'checked', 'cheek', 'cheese', 'cheeseburger', 'cheesecurds', 'chef', 'chefs', 'chewy', 'chicken', 'chinese', 'chip', 'chipotle', 'chips', 'chocolate', 'choose', 'choux', 'chow', 'cibo', 'claimed', 'classics', 'classy', 'clean', 'climbing', 'close', 'cocktail', 'cocktails', 'cod', 'coffee', 'cold', 'color', 'combination', 'combos', 'come', 'comfortable', 'coming', 'common', 'companions', 'company', 'complain', 'complaints', 'complete', 'completely', 'compliments', 'conclusion', 'condiment', 'connisseur', 'connoisseur', 'consider', 'considering', 'constructed', 'contained', 'continue', 'convenient', 'cook', 'cooked', 'cool', 'corn', 'corporation', 'cost', 'cotta', 'couldn', 'count', 'couple', 'couples', 'coupons', 'course', 'courteous', 'cover', 'covered', 'covers', 'cow', 'coziness', 'crab', 'cranberry', 'craving', 'crawfish', 'cream', 'creamy', 'crema', 'crepe', 'crisp', 'crispy', 'crostini', 'croutons', 'crumby', 'crust', 'crystals', 'cuisine', 'curry', 'customer', 'customers', 'customize', 'cut', 'cute', 'daily', 'damn', 'dark', 'date', 'dates', 'daughter', 'day', 'dead', 'deal', 'dealing', 'decent', 'decide', 'decided', 'decision', 'decor', 'decorated', 'dedicated', 'deeply', 'def', 'definitely', 'delicioso', 'delicious', 'deliciously', 'delight', 'delightful', 'delights', 'delish', 'deliver', 'delivery', 'descriptions', 'deserves', 'desired', 'despicable', 'despite', 'dessert', 'desserts', 'did', 'didn', 'die', 'difference', 'different', 'dime', 'dine', 'dining', 'dinner', 'dinners', 'dirt', 'dirty', 'disagree', 'disappoint', 'disappointed', 'disappointing', 'disappointment', 'disaster', 'discount', 'disgrace', 'disgraceful', 'disgust', 'disgusted', 'disgusting', 'dish', 'dishes', 'dispenser', 'disrespected', 'diverse', 'dog', 'dollars', 'don', 'dont', 'donut', 'door', 'dos', 'double', 'doubt', 'dough', 'downside', 'downtown', 'drag', 'drastically', 'dreamed', 'drenched', 'dressed', 'dressing', 'driest', 'drink', 'drinks', 'dripping', 'drive', 'driving', 'dropped', 'drunk', 'dry', 'duck', 'duo', 'dusted', 'dylan', 'eat', 'eaten', 'eating', 'eclectic', 'edible', 'edinburgh', 'eel', 'eew', 'efficient', 'effort', 'egg', 'eggplant', 'eggs', 'elegantly', 'elk', 'end', 'ended', 'english', 'enjoy', 'enjoyable', 'enjoyed', 'enthusiastic', 'entrees', 'equally', 'especially', 'establishment', 'ethic', 'evening', 'events', 'exactly', 'excalibur', 'exceeding', 'excellent', 'exceptional', 'excuse', 'expanded', 'expect', 'expectations', 'expected', 'expensive', 'experience', 'experienced', 'experiencing', 'expert', 'exquisite', 'extensive', 'extra', 'extraordinary', 'extremely', 'eyed', 'eyes', 'fact', 'fail', 'fails', 'fairly', 'falafels', 'falling', 'familiar', 'family', 'famous', 'fan', 'fantastic', 'far', 'fare', 'fast', 'fat', 'favor', 'favorite', 'fear', 'feel', 'feeling', 'feels', 'fellow', 'felt', 'fiancé', 'figured', 'filet', 'fillet', 'filling', 'final', 'finally', 'fine', 'finger', 'finish', 'firehouse', 'fish', 'flat', 'flavor', 'flavored', 'flavorful', 'flavors', 'flavourful', 'flirting', 'flower', 'fluffy', 'fly', 'fo', 'focused', 'folks', 'food', 'foot', 'forgetting', 'forth', 'forward', 'francisco', 'freaking', 'free', 'freezing', 'frenchman', 'fresh', 'fridays', 'fried', 'friend', 'friendly', 'friends', 'fries', 'frozen', 'frustrated', 'fry', 'fs', 'fun', 'funny', 'furthermore', 'fuzzy', 'ganoush', 'garlic', 'gas', 'gave', 'gc', 'gem', 'generic', 'generous', 'genuinely', 'getting', 'giant', 'given', 'giving', 'glad', 'glance', 'gluten', 'goat', 'godfathers', 'going', 'gold', 'gone', 'good', 'google', 'gooodd', 'gordon', 'got', 'gourmet', 'grab', 'grain', 'grandmother', 'gratitude', 'gratuity', 'grease', 'greasy', 'great', 'greatest', 'greedy', 'greek', 'green', 'greens', 'greeted', 'grill', 'grilled', 'gringos', 'gristle', 'gross', 'grossed', 'ground', 'group', 'groups', 'grow', 'guess', 'guest', 'guests', 'guy', 'guys', 'gyro', 'gyros', 'hair', 'half', 'halibut', 'han', 'hand', 'handed', 'handled', 'handling', 'handmade', 'hands', 'hankering', 'happened', 'happier', 'happy', 'hard', 'hardest', 'hate', 'haunt', 'haven', 'having', 'hawaiian', 'healthy', 'heard', 'heart', 'hearts', 'hella', 'hello', 'help', 'helped', 'helpful', 'hi', 'high', 'highlights', 'highly', 'hip', 'hiro', 'hit', 'hits', 'hole', 'holiday', 'home', 'homemade', 'honeslty', 'honest', 'honestly', 'honor', 'hooked', 'hope', 'hopefully', 'hoping', 'horrible', 'hospitality', 'host', 'hostess', 'hot', 'hottest', 'hour', 'hours', 'house', 'huge', 'human', 'humiliated', 'hummus', 'hungry', 'husband', 'hut', 'ians', 'ice', 'iced', 'ignore', 'ignored', 'im', 'imaginative', 'imagine', 'imagined', 'immediately', 'impeccable', 'impressed', 'included', 'including', 'inconsiderate', 'incredible', 'incredibly', 'indian', 'indicate', 'industry', 'inexpensive', 'inflate', 'informative', 'ingredients', 'insanely', 'inside', 'inspired', 'instead', 'insulted', 'interesting', 'interior', 'inviting', 'ironman', 'isn', 'italian', 'item', 'jamaican', 'jeff', 'jenni', 'jerk', 'jewel', 'job', 'joey', 'joint', 'joke', 'joy', 'judge', 'judging', 'juice', 'just', 'kept', 'khao', 'kiddos', 'kids', 'kind', 'kitchen', 'know', 'known', 'lack', 'lacking', 'ladies', 'lady', 'large', 'largely', 'larger', 'las', 'late', 'later', 'leave', 'leaves', 'left', 'leftover', 'legit', 'legs', 'lemon', 'let', 'letdown', 'letting', 'lettuce', 'life', 'lighter', 'lighting', 'lightly', 'like', 'liked', 'liking', 'lil', 'limited', 'lined', 'list', 'listed', 'literally', 'little', 'live', 'lived', 'living', 'll', 'lobster', 'located', 'location', 'long', 'longer', 'look', 'looked', 'looking', 'lordy', 'lot', 'lots', 'loudly', 'love', 'loved', 'lovely', 'lover', 'lovers', 'loves', 'loving', 'lox', 'luck', 'luke', 'lukewarm', 'lunch', 'mac', 'macarons', 'madhouse', 'madison', 'magazine', 'magic', 'mains', 'maintaining', 'make', 'making', 'mall', 'man', 'managed', 'management', 'manager', 'mandalay', 'mango', 'margaritas', 'marrow', 'martini', 'mary', 'massive', 'maybe', 'mayo', 'meal', 'meals', 'mean', 'means', 'meat', 'meatloaf', 'meats', 'mediocre', 'meet', 'meh', 'mein', 'melt', 'melted', 'memory', 'mention', 'menu', 'menus', 'mesquite', 'mess', 'metro', 'mexican', 'mgm', 'middle', 'military', 'milk', 'milkshake', 'min', 'mind', 'minutes', 'miss', 'missed', 'missing', 'mistake', 'mixed', 'mmmm', 'modern', 'moist', 'mojitos', 'mom', 'money', 'monster', 'months', 'mood', 'moods', 'mortified', 'mouth', 'mouthful', 'mouths', 'moz', 'muffin', 'multi', 'multiple', 'mushrooms', 'music', 'mussels', 'naan', 'nan', 'nargile', 'nasty', 'nay', 'nearly', 'need', 'needed', 'needless', 'needs', 'neighborhood', 'new', 'nice', 'nicest', 'night', 'nigiri', 'nobu', 'noca', 'non', 'noodles', 'north', 'note', 'nude', 'number', 'nut', 'nyc', 'offered', 'oh', 'ohhh', 'ok', 'old', 'olives', 'omelets', 'omg', 'onion', 'opened', 'operation', 'opinion', 'opportunity', 'options', 'order', 'ordered', 'ordering', 'orders', 'original', 'otto', 'outdoor', 'outrageously', 'outshining', 'outside', 'outstanding', 'outta', 'oven', 'overall', 'overcooked', 'overhaul', 'overpriced', 'overwhelmed', 'owner', 'owners', 'pace', 'pack', 'paid', 'palate', 'pale', 'palm', 'pan', 'pancake', 'pancakes', 'panna', 'paper', 'papers', 'par', 'paradise', 'parents', 'particular', 'parties', 'party', 'passed', 'past', 'pasta', 'pastas', 'pastry', 'patio', 'patty', 'pay', 'peanut', 'pears', 'peas', 'pecan', 'penne', 'people', 'pepper', 'perfect', 'perfection', 'perfectly', 'performed', 'perpared', 'person', 'personable', 'personally', 'petrified', 'petty', 'phenomenal', 'philadelphia', 'pho', 'phoenix', 'piano', 'picture', 'pictures', 'piece', 'pile', 'pine', 'pineapple', 'pink', 'pita', 'pizza', 'pizzas', 'place', 'places', 'plain', 'plantains', 'plate', 'plater', 'platter', 'play', 'playing', 'pleasant', 'pleased', 'plus', 'pneumatic', 'point', 'poop', 'poor', 'poorly', 'pop', 'pork', 'portion', 'portions', 'positive', 'possible', 'potato', 'potatoes', 'powdered', 'power', 'prefer', 'prepared', 'preparing', 'presentation', 'pretty', 'price', 'priced', 'prices', 'pricing', 'prime', 'privileged', 'probably', 'proclaimed', 'professional', 'profiterole', 'promise', 'prompt', 'promptly', 'pros', 'proven', 'provided', 'provides', 'providing', 'pub', 'public', 'publicly', 'pucks', 'pulled', 'pumpkin', 'puree', 'quaint', 'quality', 'quantity', 'quick', 'quickly', 'quit', 'quite', 'ramsey', 'rapidly', 'rare', 'rarely', 'raspberry', 'rate', 'rated', 'rating', 'ratio', 'raving', 'ravoli', 'readers', 'reading', 'real', 'realized', 'really', 'reasonable', 'reasonably', 'reasons', 'recall', 'received', 'receives', 'recent', 'recently', 'recommend', 'recommendation', 'recommended', 'recommending', 'red', 'redeeming', 'reduction', 'refill', 'refrained', 'refreshing', 'register', 'regular', 'regularly', 'reheated', 'relationship', 'relax', 'relaxed', 'relleno', 'relocated', 'reminded', 'reminds', 'replenished', 'requested', 'rest', 'restaurant', 'restaurants', 'return', 'returned', 'review', 'reviewer', 'reviewing', 'reviews', 'revisiting', 'rge', 'rib', 'ribeye', 'rice', 'rich', 'rick', 'right', 'rings', 'rinse', 'ripped', 'risotto', 'roast', 'roasted', 'rock', 'rolled', 'rolls', 'room', 'rowdy', 'rubber', 'rude', 'rudely', 'running', 'rushed', 'ryan', 'sad', 'sadly', 'saffron', 'saganaki', 'said', 'salad', 'salads', 'salmon', 'salt', 'salty', 'sample', 'san', 'sandwich', 'sashimi', 'sat', 'satisfied', 'satisfying', 'sauce', 'sauces', 'saving', 'say', 'says', 'scallop', 'scottsdale', 'screams', 'screwed', 'seafood', 'seasoned', 'seasoning', 'seat', 'seated', 'seating', 'second', 'section', 'seen', 'selection', 'selections', 'self', 'send', 'sense', 'sergeant', 'seriously', 'serivce', 'serve', 'served', 'server', 'servers', 'serves', 'service', 'serving', 'set', 'setting', 'sever', 'sexy', 'shall', 'sharply', 'shawarrrrrrma', 'shirt', 'shocked', 'shoots', 'shop', 'shopping', 'shops', 'short', 'shouldn', 'showed', 'shower', 'shrimp', 'sick', 'sides', 'sign', 'signs', 'silently', 'similar', 'similarly', 'simple', 'simply', 'single', 'sit', 'sitting', 'skimp', 'slaw', 'sliced', 'slices', 'slow', 'small', 'smaller', 'smashburger', 'smeared', 'smelled', 'smooth', 'smoothies', 'soggy', 'soi', 'solid', 'somethat', 'somewhat', 'son', 'songs', 'soon', 'soooo', 'sooooo', 'soooooo', 'sore', 'sorely', 'sorry', 'sound', 'soundtrack', 'soup', 'southwest', 'space', 'spaghetti', 'special', 'specials', 'speedy', 'spends', 'spicier', 'spicy', 'spinach', 'sporting', 'spot', 'spots', 'spring', 'sprouts', 'staff', 'stale', 'standard', 'star', 'stars', 'starving', 'station', 'stay', 'stayed', 'staying', 'steak', 'steaks', 'steiners', 'step', 'stepped', 'steve', 'sticks', 'stinks', 'stir', 'stomach', 'stood', 'stop', 'stopped', 'strange', 'strangers', 'street', 'strike', 'strings', 'strip', 'struck', 'struggle', 'stuff', 'stuffed', 'stupid', 'styrofoam', 'sub', 'subpar', 'subway', 'succulent', 'suck', 'sucked', 'sucker', 'sucks', 'suffers', 'sugar', 'sugary', 'suggestions', 'summarize', 'summary', 'summer', 'sun', 'sunglasses', 'super', 'supposed', 'sure', 'sushi', 'sweet', 'swung', 'table', 'tables', 'taco', 'tacos', 'tailored', 'talk', 'talking', 'tap', 'tapas', 'tartar', 'tartare', 'taste', 'tasted', 'tasteless', 'tastings', 'tasty', 'tater', 'tea', 'teeth', 'tell', 'tender', 'tenders', 'terrible', 'terrific', 'texture', 'thai', 'thanks', 'theft', 'thing', 'things', 'think', 'thinly', 'thirty', 'thoroughly', 'thought', 'thrilled', 'thumbs', 'tigerlilly', 'time', 'times', 'tiny', 'tip', 'tiramisu', 'toast', 'toasted', 'today', 'told', 'tongue', 'tonight', 'took', 'topic', 'toro', 'total', 'totally', 'tots', 'touch', 'touched', 'tough', 'town', 'tracked', 'tragedy', 'transcendant', 'trap', 'treat', 'treated', 'tried', 'trimmed', 'trip', 'trips', 'truffle', 'truly', 'try', 'trying', 'tucson', 'tummy', 'tuna', 'turkey', 'turn', 'tv', 'twice', 'typical', 'unbelievable', 'unbelievably', 'underwhelming', 'unfortunately', 'unhealthy', 'uninspired', 'unless', 'unreal', 'unsatisfying', 'untoasted', 'update', 'upgrading', 'uploaded', 'use', 'used', 'usual', 'vacant', 'vain', 'valley', 'value', 'vanilla', 've', 'vegan', 'vegas', 'vegetables', 'vegetarian', 'veggie', 'veggitarian', 'velvet', 'ventilation', 'venture', 'venturing', 'venue', 'verge', 'vibe', 'vinaigrette', 'vinegrette', 'violinists', 'visit', 'visited', 'vodka', 'voodoo', 'voted', 'wagyu', 'wait', 'waited', 'waiter', 'waiting', 'waitress', 'waitresses', 'walked', 'wall', 'walls', 'want', 'wanted', 'wants', 'warm', 'warmer', 'wasn', 'waste', 'wasting', 'watch', 'watched', 'water', 'wave', 'way', 'ways', 'wayyy', 'website', 'wedges', 'week', 'weekend', 'weekly', 'welcome', 'went', 'weren', 'whatsoever', 'white', 'wide', 'wife', 'wildly', 'wine', 'wings', 'winner', 'wish', 'witnessed', 'won', 'wonderful', 'word', 'work', 'worker', 'working', 'world', 'worries', 'worse', 'worst', 'worth', 'wouldn', 'wow', 'wrap', 'wrapped', 'writing', 'wrong', 'yama', 'yeah', 'year', 'years', 'yellow', 'yellowtail', 'yelpers', 'yucky', 'yukon', 'yum', 'yummy', 'zero']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "print(len(feature_names))\n",
    "print(feature_names)\n",
    "#print(train_vector_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "def write_terms (feature_names, data, vector_data, index):\n",
    "    '''\n",
    "    Escribe los términos presentes en un mensaje representado como bolsa de palabras.\n",
    "    \n",
    "    - feature_names: terminos usados para vectorizar\n",
    "    - data: lista de mensajes original (si data==None no se muestra el mensaje original)\n",
    "    - vector_data: matriz (dispersa) de mensaje vectorizados\n",
    "    - index: posición del mensaje a mostrar\n",
    "    '''\n",
    "    # máscara para seleccionar sólo el mensaje en posición index\n",
    "    mask=vector_data[index,:]>0\n",
    "    \n",
    "    # términos que aparecen en ese mensaje vectorizado\n",
    "    terminos = ma.array(feature_names, mask = ~(mask[0].toarray()))\n",
    "    \n",
    "    # mostrar mensaje original\n",
    "    if data:\n",
    "        print('Mensaje', index, ':', data[index])\n",
    "    \n",
    "    # mostrar términos que aparecen en el mensaje vectorizado\n",
    "    print('Mensaje', index, 'vectorizado:', terminos.compressed(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mensaje 0 vectorizado: ['loved' 'place' 'wow'] \n",
      "\n",
      "Mensaje 10 vectorizado: ['prompt' 'service'] \n",
      "\n",
      "Mensaje 100 vectorizado: ['added' 'bone' 'extra' 'fantastic' 'garlic' 'loves' 'marrow' 'meal'\n",
      " 'roasted' 'server' 'wife'] \n",
      "\n",
      "Mensaje 200 vectorizado: ['dreamed' 'exceeding' 'good' 'heard' 'hope' 'place' 'things'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#write_terms(feature_names, train_data.data, train_vector_data, 10)\n",
    "write_terms(feature_names, None, train_vector_data, 0)\n",
    "write_terms(feature_names, None, train_vector_data, 10)\n",
    "\n",
    "write_terms(feature_names, None, train_vector_data, 100)\n",
    "write_terms(feature_names, None, train_vector_data, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=lines_org, stop_words='english')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Duplicate term in vocabulary: 'I love this place.\\t1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-07750a406513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;34m\"\"\"Array mapping from feature integer indices to feature name\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocabulary_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_validate_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Duplicate term in vocabulary: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m                 \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Duplicate term in vocabulary: 'I love this place.\\t1'"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfer = TfidfTransformer()\n",
    "train_preprocessed = tfidfer.fit_transform(train_vector_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mensaje 10 vectorizado: ['prompt' 'service'] \n",
      "\n",
      "Mensaje 100 vectorizado: ['added' 'bone' 'extra' 'fantastic' 'garlic' 'loves' 'marrow' 'meal'\n",
      " 'roasted' 'server' 'wife'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#write_terms(feature_names, train_data.data, train_vector_data, 10)\n",
    "write_terms(feature_names, None, train_vector_data, 10)\n",
    "\n",
    "write_terms(feature_names, None, train_vector_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-611cff899cd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# de palabras. Al usar \"transform\" toma como referencia únicamente las palabras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# encontradas en el conjunto de entrenamiento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_vector_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# Calculamos el valor TF-IDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Al usar \"transform\" toma como IDF el del conjunto de entrenamiento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Tomamos los textos del conjunto de test y los transformamos en una matriz\n",
    "# de palabras. Al usar \"transform\" toma como referencia únicamente las palabras\n",
    "# encontradas en el conjunto de entrenamiento\n",
    "test_vector_data=vectorizer.transform(test_data.data)\n",
    "# Calculamos el valor TF-IDF \n",
    "# Al usar \"transform\" toma como IDF el del conjunto de entrenamiento \n",
    "test_preprocessed=tfidfer.transform(test_vector_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    \n",
    "#### Para cada una de esas 4 combinaciones entrenaremos dos clasificadores:\n",
    "\n",
    "     1. Un clasificador naive bayes, eligiendo el más adecuado para cada caso.\n",
    "     \n",
    "     2. Un árbol de decisión buscando un valor óptimo para uno de los siguientes parámetros para que se maximice la tasa de aciertos en el conjunto de test: max_depth, min_samples_leaf o max_leaf_nodes (siempre el mismo).\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analiza la tasa de aciertos de entrenamiento y test de los 2 clasificadores en las 4 representaciones de bolsa de palabras (8 configuraciones en total) y contesta a las siguientes preguntas:\n",
    "\n",
    "    - ¿Hay un clasificador que sea superior al otro? ¿por qué crees que sucede?\n",
    "    \n",
    "    - Para cada clasificador, ¿tiene un efecto positivo el añadir “complejidad” a la vectorización? Es decir, añadir bigramas y añadir tf-idf. ¿Por qué crees que sucede este efecto positivo o la falta del mismo? \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecciona el mejor árbol de decisión y obtén las 25 variables con más poder discriminante:\n",
    "    \n",
    "    - ¿Predominan más las palabras de uno u otro sentimiento? ¿por qué? ¿hay ruido? \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecciona el mejor clasificador naive bayes y obtén las 25 variables con más presencia en cada clase:\n",
    "    \n",
    "    - ¿Tienen sentido las palabras seleccionadas? ¿hay ruido (palabras sin sentimiento o de sentimiento opuesto al esperado)? ¿por qué crees que suceden estos fenómenos?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalmente, explica de manera razonada las conclusiones que has extraído de todo el estudio realizado en este apartado.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apartado b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toma el mejor clasificador Naive Bayes y el mejor árbol de decisión y analiza a fondo sus resultados en el conjunto de test.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Analiza la precisión y la exhaustividad de cada clasificador en cada una de las clases (opiniones positivas y negativas).\n",
    "    \n",
    "         Para cada clasificador, ¿tiene un comportamiento homogéneo a la hora de clasificar ambas clases?\n",
    "         ¿Cuáles son las fortalezas y debilidades de cada uno de los clasificadores?\n",
    "         ¿Hay algún clasificador que sea mejor que el otro en todo?\n",
    "         ¿Coinciden ambos clasificadores a la hora de clasificar mejor una clase que la otra?\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Pinta los 8 primeros niveles del árbol de decisión y comenta lo que ves.\n",
    "         ¿Qué estructura tiene el árbol?\n",
    "         ¿Cómo interpretas los niveles que has pintado? ¿tienen algún sentido con respecto a la tasa de aciertos, o la precisión y exhaustividad del clasificador? o ¿Hay nodos impuros?\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Por cada clasificador identifica 2 críticas que hayan sido falsas positivas (malas críticas calificadas como buenas) y 2 críticas que han sido falsas negativas (buenas críticas clasificadas como malas). Analiza tanto su texto original, como el vector de palabras resultante (solamente los términos activos).\n",
    "         ¿Por qué crees que ha fallado el clasificador en cada uno de los casos?\n",
    "         ¿Se te ocurre alguna idea sobre cómo mejorar el clasificador de sentimiento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

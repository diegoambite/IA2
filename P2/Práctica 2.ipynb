{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica 2. Procesamiento de lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 1. Análisis de sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"yelp_labelled.txt\",\"r\") as text_file:\n",
    "    lines_org = text_file.read().split('\\n')\n",
    "len(lines_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [line.split(\"\\t\") for line in lines_org if len(line.split(\"\\t\"))==2 and line.split(\"\\t\")[1]!='']\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_line = np.array(lines)\n",
    "text = data_line[:,0]\n",
    "target = data_line[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Texto':text,'Target':target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texto</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Texto Target\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Texto', 'Target'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apartado a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configura una partición train-test usando el 75% de los datos para entrenamiento y el 25% restante para test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Texto'], df['Target'], test_size=0.25, random_state=333)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos a estudiar varias representaciones de bolsa de palabras, pero todas ellas utilizarán countVectorizer con el diccionario que se crea a partir de los términos del propio corpus y la lista de palabras vacías (stop_words) que proporciona sklearn para el inglés. Las 4 posibilidades que estudiaremos surgen de combinar los siguientes 2 parámetros:\n",
    "\n",
    "    - Bolsa de palabras binaria y bolsa de palabras con TF/IDF (parámetro binary).\n",
    "    \n",
    "    - Usando un rango de n-gramas de (1,1) y de (1,2) (parámetro ngram_range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "# Tomamos los textos del conjunto de entrenamiento y los transformamos en \n",
    "# una matriz de datos (palabras) según el diccionario estándar\n",
    "train_vector_data=vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 440)\t1\n",
      "  (0, 448)\t1\n",
      "  (0, 516)\t1\n",
      "  (0, 859)\t1\n",
      "  (0, 1074)\t1\n",
      "  (1, 175)\t1\n",
      "  (1, 710)\t1\n",
      "  (1, 1412)\t1\n",
      "  (2, 86)\t1\n",
      "  (2, 506)\t1\n",
      "  (2, 744)\t1\n",
      "  (2, 1152)\t1\n",
      "  (2, 1256)\t1\n",
      "  (3, 958)\t1\n",
      "  (3, 1244)\t1\n",
      "  (3, 1271)\t1\n",
      "  (4, 101)\t1\n",
      "  (4, 587)\t1\n",
      "  (4, 732)\t1\n",
      "  (5, 615)\t1\n",
      "  (5, 1087)\t1\n",
      "  (6, 269)\t1\n",
      "  (6, 940)\t1\n",
      "  (6, 1226)\t1\n",
      "  (6, 1338)\t1\n",
      "  :\t:\n",
      "  (744, 120)\t1\n",
      "  (744, 803)\t1\n",
      "  (744, 1083)\t1\n",
      "  (745, 127)\t1\n",
      "  (745, 749)\t1\n",
      "  (745, 772)\t1\n",
      "  (745, 1197)\t1\n",
      "  (745, 1434)\t1\n",
      "  (746, 360)\t1\n",
      "  (747, 423)\t1\n",
      "  (747, 521)\t1\n",
      "  (747, 744)\t1\n",
      "  (747, 755)\t1\n",
      "  (747, 856)\t1\n",
      "  (747, 860)\t1\n",
      "  (747, 900)\t1\n",
      "  (747, 1000)\t1\n",
      "  (747, 1149)\t1\n",
      "  (747, 1223)\t1\n",
      "  (748, 506)\t1\n",
      "  (748, 553)\t1\n",
      "  (748, 1412)\t1\n",
      "  (749, 447)\t1\n",
      "  (749, 990)\t1\n",
      "  (749, 1120)\t1\n"
     ]
    }
   ],
   "source": [
    "print(train_vector_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472\n",
      "['00', '10', '100', '12', '15', '17', '1979', '20', '2007', '23', '30', '35', '40', '40min', '45', '4ths', '5lb', '85', '90', 'absolutely', 'absolutley', 'accomodate', 'accordingly', 'accountant', 'ache', 'acknowledged', 'actual', 'actually', 'added', 'afternoon', 'ago', 'airline', 'albondigas', 'allergy', 'almonds', 'amazing', 'ambiance', 'ambience', 'andddd', 'annoying', 'anymore', 'anytime', 'anyways', 'apologize', 'apology', 'app', 'appalling', 'appealing', 'appetite', 'appetizer', 'appetizers', 'apple', 'approval', 'area', 'arepas', 'aria', 'array', 'arrived', 'arriving', 'article', 'ask', 'asked', 'asking', 'assure', 'ate', 'atmosphere', 'atrocious', 'attached', 'attentive', 'attitudes', 'auju', 'authentic', 'average', 'avoid', 'avoided', 'away', 'awesome', 'awful', 'awkward', 'awkwardly', 'ayce', 'az', 'baba', 'baby', 'bachi', 'bacon', 'bad', 'bagels', 'bakery', 'baklava', 'ball', 'bamboo', 'banana', 'bank', 'bar', 'barely', 'bargain', 'bartender', 'bartenders', 'baseball', 'based', 'basically', 'bathroom', 'bathrooms', 'batter', 'bay', 'bbq', 'bean', 'beans', 'beat', 'beateous', 'beautiful', 'beauty', 'beef', 'beer', 'beers', 'begin', 'believe', 'bellies', 'belly', 'best', 'better', 'big', 'biggest', 'binge', 'biscuit', 'biscuits', 'bisque', 'bit', 'bite', 'bites', 'black', 'blah', 'bland', 'blandest', 'block', 'bloddy', 'bloodiest', 'blow', 'blown', 'blows', 'boba', 'boiled', 'bone', 'boot', 'boring', 'bouchon', 'bought', 'bowl', 'boxes', 'boy', 'boyfriend', 'boys', 'bread', 'break', 'breakfast', 'breaks', 'breeze', 'brick', 'bring', 'brings', 'brisket', 'brought', 'brunch', 'bruschetta', 'brushfire', 'buffet', 'building', 'buldogis', 'burger', 'burgers', 'burned', 'burrittos', 'business', 'businesses', 'busy', 'butter', 'buying', 'bye', 'caballero', 'cafe', 'café', 'cake', 'cakes', 'calamari', 'calligraphy', 'came', 'cannoli', 'car', 'carbs', 'care', 'caring', 'carly', 'carpaccio', 'case', 'casino', 'caterpillar', 'caught', 'cavier', 'certainly', 'chai', 'chains', 'changing', 'char', 'charcoal', 'charming', 'cheap', 'cheated', 'check', 'checked', 'cheek', 'cheese', 'cheesecurds', 'chef', 'chefs', 'chewy', 'chicken', 'chinese', 'chips', 'chocolate', 'choose', 'choux', 'chow', 'christmas', 'cibo', 'circumstances', 'class', 'classic', 'classics', 'classy', 'clean', 'closed', 'club', 'clue', 'cocktail', 'cocktails', 'coconut', 'coffee', 'cold', 'colder', 'college', 'color', 'combination', 'combos', 'come', 'comfortable', 'coming', 'common', 'companions', 'company', 'complain', 'complaints', 'complete', 'completely', 'compliments', 'concept', 'concern', 'condiment', 'connisseur', 'connoisseur', 'consider', 'considering', 'consistent', 'constructed', 'contain', 'contained', 'containers', 'continue', 'convenient', 'cooked', 'cooking', 'cool', 'corn', 'corporation', 'correct', 'correction', 'cost', 'costco', 'cotta', 'couldn', 'couple', 'couples', 'coupons', 'course', 'court', 'courteous', 'covered', 'covers', 'cow', 'coziness', 'crab', 'cramming', 'craving', 'crawfish', 'crazy', 'cream', 'creamy', 'crema', 'crepe', 'crisp', 'crispy', 'crostini', 'croutons', 'crowds', 'crumby', 'crust', 'crystals', 'cuisine', 'curry', 'customer', 'customers', 'customize', 'cut', 'cute', 'daily', 'damn', 'dark', 'date', 'dates', 'day', 'dead', 'deal', 'dealing', 'decent', 'decided', 'decor', 'decorated', 'dedicated', 'deeply', 'def', 'definately', 'definitely', 'degree', 'del', 'delicious', 'deliciously', 'delight', 'delightful', 'delish', 'delivery', 'denny', 'descriptions', 'deserves', 'despicable', 'despite', 'dessert', 'desserts', 'devine', 'did', 'didn', 'die', 'difference', 'different', 'dime', 'dine', 'dining', 'dinner', 'dirt', 'dirty', 'disappoint', 'disappointed', 'disappointing', 'disappointment', 'disaster', 'disbelief', 'disgrace', 'disgraceful', 'disgust', 'disgusted', 'disgusting', 'dish', 'dishes', 'dispenser', 'disrespected', 'diverse', 'does', 'dog', 'dollars', 'don', 'dont', 'donut', 'door', 'dos', 'doubt', 'douchey', 'downright', 'downside', 'downtown', 'drag', 'drastically', 'drawing', 'dreamed', 'dressed', 'dressing', 'driest', 'drink', 'drinking', 'drinks', 'dripping', 'drive', 'dropped', 'drunk', 'dry', 'duck', 'duo', 'dylan', 'easily', 'eat', 'eaten', 'eating', 'eclectic', 'edible', 'editing', 'eel', 'eew', 'effort', 'egg', 'eggplant', 'eggs', 'elegantly', 'elk', 'email', 'employee', 'end', 'ended', 'english', 'enjoy', 'enjoyable', 'enjoyed', 'ensued', 'enthusiastic', 'entire', 'especially', 'establishment', 'eve', 'evening', 'events', 'exactly', 'excalibur', 'exceeding', 'excellent', 'exceptional', 'excuse', 'expanded', 'expect', 'expected', 'expensive', 'experience', 'experienced', 'experiencing', 'expert', 'extensive', 'extra', 'extraordinary', 'extremely', 'eyed', 'eyes', 'fact', 'fail', 'fair', 'fairly', 'falafels', 'familiar', 'family', 'fan', 'fantastic', 'far', 'fare', 'fast', 'fat', 'fav', 'favor', 'favorite', 'fear', 'feel', 'feeling', 'feels', 'fell', 'fella', 'felt', 'fiancé', 'figured', 'filet', 'fillet', 'filling', 'final', 'finally', 'fine', 'finger', 'finish', 'firehouse', 'fish', 'flair', 'flat', 'flavor', 'flavored', 'flavorful', 'flirting', 'flop', 'flower', 'fluffy', 'fly', 'fo', 'focused', 'folks', 'fondue', 'food', 'foods', 'foot', 'forever', 'forgetting', 'forth', 'forward', 'freaking', 'free', 'freezing', 'frenchman', 'fresh', 'fridays', 'fried', 'friend', 'friendly', 'friends', 'fries', 'frozen', 'fruit', 'fry', 'fs', 'fucking', 'fun', 'funny', 'furthermore', 'fuzzy', 'ganoush', 'garden', 'garlic', 'gas', 'gave', 'gem', 'generic', 'generous', 'genuinely', 'gets', 'getting', 'girlfriend', 'given', 'glass', 'gluten', 'godfathers', 'going', 'gold', 'golden', 'gone', 'good', 'google', 'gooodd', 'gordon', 'got', 'gotten', 'gourmet', 'grab', 'grain', 'grandmother', 'gratitude', 'gratuity', 'grease', 'greasy', 'great', 'greedy', 'greek', 'green', 'greens', 'greeted', 'grill', 'grilled', 'gringos', 'gristle', 'grocery', 'gross', 'grossed', 'group', 'groups', 'grow', 'guacamole', 'guess', 'guests', 'guys', 'gyro', 'gyros', 'ha', 'hadn', 'hair', 'half', 'halibut', 'hamburger', 'handmade', 'hands', 'hankering', 'happened', 'happy', 'hard', 'hardly', 'hasn', 'hate', 'hated', 'haunt', 'haven', 'hawaiian', 'healthy', 'heard', 'heart', 'hearts', 'heimer', 'hell', 'hella', 'hello', 'help', 'helped', 'helpful', 'high', 'highlight', 'highlighted', 'highlights', 'highly', 'hilarious', 'hip', 'hit', 'hole', 'holiday', 'home', 'homemade', 'honeslty', 'honest', 'honestly', 'honor', 'hooked', 'hope', 'hopes', 'hoping', 'horrible', 'hospitality', 'hostess', 'hot', 'hottest', 'hour', 'hours', 'house', 'huevos', 'huge', 'human', 'hummus', 'hungry', 'hurry', 'husband', 'hut', 'ians', 'ice', 'iced', 'idea', 'ignored', 'im', 'imagination', 'imagine', 'imagined', 'immediately', 'impeccable', 'impressed', 'impressive', 'inch', 'incredible', 'indian', 'indoor', 'industry', 'inexpensive', 'inflate', 'informative', 'insanely', 'inside', 'inspired', 'instantly', 'instead', 'insulted', 'interesting', 'interior', 'inviting', 'ironman', 'isn', 'italian', 'item', 'jamaican', 'jenni', 'jewel', 'job', 'join', 'joint', 'joke', 'joy', 'judge', 'judging', 'juice', 'juries', 'just', 'kabuki', 'kept', 'khao', 'kid', 'kiddos', 'kids', 'killer', 'kind', 'know', 'known', 'lacked', 'lacking', 'ladies', 'lady', 'largely', 'larger', 'las', 'lastly', 'late', 'later', 'latte', 'lawyers', 'leather', 'leave', 'left', 'leftover', 'legit', 'legs', 'lemon', 'let', 'lettuce', 'level', 'life', 'light', 'lighter', 'like', 'liked', 'likes', 'lil', 'limited', 'list', 'literally', 'little', 'live', 'lived', 'living', 'll', 'lobster', 'located', 'location', 'long', 'longer', 'look', 'looked', 'looks', 'lordy', 'lost', 'lot', 'lots', 'loudly', 'love', 'loved', 'lovely', 'lover', 'lovers', 'loves', 'loving', 'low', 'loyal', 'luck', 'lukewarm', 'lunch', 'mac', 'macarons', 'madhouse', 'madison', 'magic', 'maine', 'mains', 'maintaining', 'make', 'making', 'mall', 'man', 'managed', 'management', 'manager', 'mandalay', 'mango', 'margaritas', 'market', 'marrow', 'martini', 'mary', 'maybe', 'mayo', 'meal', 'meals', 'means', 'meat', 'meatballs', 'mediocre', 'mediterranean', 'medium', 'meh', 'mein', 'mellow', 'melt', 'melted', 'memory', 'mention', 'menu', 'metro', 'mexican', 'middle', 'mile', 'milk', 'milkshake', 'min', 'mind', 'minutes', 'mirage', 'missed', 'mistake', 'mixed', 'modern', 'moist', 'mojitos', 'mom', 'money', 'monster', 'months', 'mortified', 'mouth', 'mouthful', 'mouths', 'movies', 'mozzarella', 'muffin', 'multi', 'multiple', 'mushroom', 'mushrooms', 'music', 'naan', 'nachos', 'nargile', 'nasty', 'nay', 'nearly', 'neat', 'need', 'needed', 'needless', 'needs', 'negligent', 'neighborhood', 'new', 'nice', 'nicest', 'night', 'nigiri', 'ninja', 'non', 'note', 'nude', 'nut', 'nutshell', 'occasions', 'offered', 'offers', 'officially', 'oh', 'ohhh', 'ok', 'old', 'olives', 'omg', 'operation', 'opportunity', 'opposed', 'options', 'order', 'ordered', 'ordering', 'orders', 'otto', 'outdoor', 'outrageously', 'outshining', 'outside', 'outstanding', 'outta', 'oven', 'overall', 'overcooked', 'overhaul', 'overpriced', 'overwhelmed', 'owner', 'owners', 'oysters', 'pace', 'pack', 'packed', 'paid', 'palate', 'pale', 'palm', 'pan', 'pancakes', 'panna', 'paper', 'papers', 'par', 'paradise', 'particular', 'parties', 'party', 'passed', 'past', 'pasta', 'pastry', 'patio', 'patron', 'pay', 'paying', 'peach', 'peanut', 'peanuts', 'pears', 'peas', 'pecan', 'penne', 'people', 'pepper', 'perfect', 'perfection', 'perfectly', 'performed', 'person', 'personable', 'personally', 'petrified', 'petty', 'phenomenal', 'pho', 'phoenix', 'piano', 'piece', 'pine', 'pineapple', 'pink', 'pita', 'pizza', 'pizzas', 'place', 'places', 'plain', 'plantains', 'plastic', 'plate', 'plater', 'platter', 'play', 'playing', 'pleasant', 'pleased', 'pleasure', 'plethora', 'plus', 'pneumatic', 'point', 'poisoning', 'polite', 'poor', 'poorly', 'pop', 'pork', 'portion', 'portions', 'positive', 'possible', 'potatoes', 'poured', 'power', 'prefer', 'preparing', 'pretty', 'price', 'priced', 'prices', 'pricey', 'pricing', 'prime', 'privileged', 'probably', 'proclaimed', 'professional', 'profiterole', 'promise', 'promptly', 'proven', 'provided', 'provides', 'providing', 'pub', 'public', 'publicly', 'pucks', 'pulled', 'pumpkin', 'puree', 'puréed', 'putting', 'quaint', 'qualified', 'quality', 'quick', 'quickly', 'quit', 'quite', 'ramsey', 'rancheros', 'rapidly', 'rare', 'rarely', 'raspberry', 'rate', 'rated', 'rating', 'ratio', 'rave', 'raving', 'read', 'reading', 'real', 'realized', 'really', 'reason', 'reasonable', 'reasonably', 'received', 'receives', 'recent', 'recently', 'recommend', 'recommendation', 'recommended', 'recommending', 'red', 'redeeming', 'refill', 'refrained', 'refreshing', 'refused', 'register', 'regular', 'regularly', 'reheated', 'relationship', 'relax', 'relaxed', 'relleno', 'relocated', 'remember', 'reminded', 'replenished', 'requested', 'reservation', 'rest', 'restaraunt', 'restaurant', 'return', 'returned', 'returning', 'review', 'reviews', 'rge', 'ri', 'rib', 'rice', 'rich', 'rick', 'ridiculous', 'right', 'rinse', 'ripped', 'roast', 'roasted', 'rock', 'roll', 'rolled', 'rolls', 'room', 'rotating', 'round', 'rude', 'rudely', 'running', 'rushed', 'sad', 'sadly', 'saffron', 'saganaki', 'said', 'salad', 'salads', 'salmon', 'salsa', 'salt', 'salty', 'sample', 'sandwich', 'sandwiches', 'sangria', 'sashimi', 'sat', 'satifying', 'satisfied', 'satisfying', 'sauce', 'sauces', 'saving', 'say', 'saying', 'scallop', 'screwed', 'seafood', 'seal', 'seasonal', 'seasoned', 'seasoning', 'seat', 'seated', 'seating', 'second', 'section', 'seen', 'selection', 'selections', 'self', 'sense', 'sergeant', 'seriously', 'serve', 'served', 'server', 'servers', 'serves', 'service', 'services', 'serving', 'setting', 'sewer', 'sexy', 'shall', 'sharply', 'shawarrrrrrma', 'shoe', 'shoots', 'shopping', 'short', 'shouldn', 'shower', 'shrimp', 'sick', 'sides', 'sign', 'similarly', 'simple', 'single', 'sit', 'sitting', 'sliced', 'slow', 'small', 'smaller', 'smashburger', 'smells', 'smooth', 'smoothies', 'soggy', 'soi', 'somewhat', 'son', 'songs', 'soon', 'soooo', 'sooooo', 'soooooo', 'sore', 'sorely', 'sound', 'soundtrack', 'soup', 'soups', 'sour', 'southwest', 'space', 'special', 'specials', 'spend', 'spice', 'spices', 'spicier', 'spicy', 'sporting', 'spot', 'spots', 'spotty', 'spring', 'staff', 'stale', 'standard', 'star', 'stars', 'started', 'starving', 'station', 'stay', 'stayed', 'staying', 'steak', 'steaks', 'steiners', 'step', 'stepped', 'steve', 'sticks', 'stinks', 'stir', 'stomach', 'stood', 'stop', 'stopped', 'store', 'strange', 'street', 'stretch', 'strike', 'strings', 'strip', 'struck', 'stuff', 'stuffed', 'stupid', 'style', 'styrofoam', 'sub', 'succulent', 'suck', 'sucked', 'sucker', 'sucks', 'suffers', 'sugary', 'suggest', 'suggestions', 'summarize', 'summary', 'summer', 'sun', 'sunday', 'sunglasses', 'super', 'sure', 'surprise', 'surprised', 'sushi', 'sweet', 'swung', 'table', 'tables', 'taco', 'tacos', 'tailored', 'takeout', 'talk', 'tap', 'tapas', 'tartar', 'tartare', 'taste', 'tasted', 'tasteless', 'tasty', 'tater', 'tea', 'teamwork', 'teeth', 'tell', 'tender', 'tenders', 'terrible', 'texture', 'thai', 'thanks', 'thats', 'theft', 'thing', 'things', 'think', 'thinly', 'thoroughly', 'thought', 'thumbs', 'tigerlilly', 'time', 'times', 'tiny', 'tip', 'tiramisu', 'toasted', 'today', 'told', 'tolerance', 'tomato', 'tongue', 'tonight', 'took', 'topic', 'tops', 'toro', 'total', 'totally', 'tots', 'touch', 'touched', 'tough', 'town', 'tragedy', 'transcendant', 'treat', 'treated', 'tried', 'trimmed', 'trip', 'trippy', 'trips', 'truffle', 'try', 'trying', 'tummy', 'turkey', 'turn', 'tv', 'twice', 'typical', 'unbelievable', 'undercooked', 'underwhelming', 'unfortunately', 'unhealthy', 'uninspired', 'unique', 'unless', 'unprofessional', 'unreal', 'unsatisfying', 'untoasted', 'unwelcome', 'unwrapped', 'update', 'upgrading', 'use', 'usual', 'vain', 'valley', 'value', 'vanilla', 've', 'veal', 'vegan', 'vegas', 'vegetables', 'vegetarian', 'veggie', 'veggitarian', 'velvet', 'ventilation', 'venture', 'venturing', 'venue', 'version', 'vibe', 'vinaigrette', 'vinegrette', 'violinists', 'visit', 'visited', 'vodka', 'voodoo', 'waaaaaayyyyyyyyyy', 'wagyu', 'wait', 'waited', 'waiter', 'waiting', 'waitress', 'waitresses', 'walked', 'wall', 'walls', 'want', 'wanted', 'wants', 'warm', 'warnings', 'wash', 'wasn', 'waste', 'wasted', 'wasting', 'watch', 'watched', 'water', 'way', 'ways', 'weak', 'website', 'wedges', 'week', 'weekly', 'weird', 'welcome', 'went', 'weren', 'whelm', 'white', 'wide', 'wienerschnitzel', 'wife', 'wildly', 'wine', 'wines', 'wings', 'winner', 'wire', 'witnessed', 'won', 'wonderful', 'wontons', 'word', 'work', 'workers', 'working', 'world', 'worries', 'worse', 'worst', 'worth', 'wouldn', 'wound', 'wow', 'wrap', 'wrapped', 'writing', 'wrong', 'ya', 'yama', 'year', 'years', 'yellow', 'yellowtail', 'yucky', 'yukon', 'yum', 'yummy', 'zero']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "print(len(feature_names))\n",
    "print(feature_names)\n",
    "#print(train_vector_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "def write_terms (feature_names, data, vector_data, index):\n",
    "    '''\n",
    "    Escribe los términos presentes en un mensaje representado como bolsa de palabras.\n",
    "    \n",
    "    - feature_names: terminos usados para vectorizar\n",
    "    - data: lista de mensajes original (si data==None no se muestra el mensaje original)\n",
    "    - vector_data: matriz (dispersa) de mensaje vectorizados\n",
    "    - index: posición del mensaje a mostrar\n",
    "    '''\n",
    "    # máscara para seleccionar sólo el mensaje en posición index\n",
    "    mask=vector_data[index,:]>0\n",
    "    \n",
    "    # términos que aparecen en ese mensaje vectorizado\n",
    "    terminos = ma.array(feature_names, mask = ~(mask[0].toarray()))\n",
    "    \n",
    "    # mostrar mensaje original\n",
    "    if data:\n",
    "        print('Mensaje', index, ':', data[index])\n",
    "    \n",
    "    # mostrar términos que aparecen en el mensaje vectorizado\n",
    "    print('Mensaje', index, 'vectorizado:', terminos.compressed(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mensaje 0 vectorizado: ['excellent' 'experienced' 'frenchman' 'new' 'restaurant'] \n",
      "\n",
      "Mensaje 10 vectorizado: ['caballero' 'recently' 'tried' 'week'] \n",
      "\n",
      "Mensaje 100 vectorizado: ['moist' 'shrimp' 'tender'] \n",
      "\n",
      "Mensaje 200 vectorizado: ['disgusted' 'hair' 'human' 'pretty' 'sure'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#write_terms(feature_names, train_data.data, train_vector_data, 10)\n",
    "write_terms(feature_names, None, train_vector_data, 0)\n",
    "write_terms(feature_names, None, train_vector_data, 10)\n",
    "\n",
    "write_terms(feature_names, None, train_vector_data, 100)\n",
    "write_terms(feature_names, None, train_vector_data, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el valor TF-IDF \n",
    "tfidfer = TfidfTransformer()\n",
    "train_preprocessed = tfidfer.fit_transform(train_vector_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mensaje 10 vectorizado: ['caballero' 'recently' 'tried' 'week'] \n",
      "\n",
      "Mensaje 100 vectorizado: ['moist' 'shrimp' 'tender'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#write_terms(feature_names, train_data.data, train_vector_data, 10)\n",
    "write_terms(feature_names, None, train_vector_data, 10)\n",
    "\n",
    "write_terms(feature_names, None, train_vector_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomamos los textos del conjunto de test y los transformamos en una matriz\n",
    "# de palabras. Al usar \"transform\" toma como referencia únicamente las palabras\n",
    "# encontradas en el conjunto de entrenamiento\n",
    "test_vector_data=vectorizer.transform(X_test)\n",
    "# Calculamos el valor TF-IDF \n",
    "# Al usar \"transform\" toma como IDF el del conjunto de entrenamiento \n",
    "test_preprocessed=tfidfer.transform(test_vector_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 241)\t1\n",
      "  (0, 517)\t1\n",
      "  (0, 1118)\t1\n",
      "  (0, 1219)\t1\n",
      "  (0, 1412)\t1\n",
      "  (1, 331)\t1\n",
      "  (1, 626)\t1\n",
      "  (1, 748)\t1\n",
      "  (1, 1210)\t1\n",
      "  (1, 1332)\t1\n",
      "  (2, 95)\t1\n",
      "  (2, 159)\t1\n",
      "  (2, 506)\t1\n",
      "  (2, 772)\t1\n",
      "  (2, 1149)\t1\n",
      "  (2, 1175)\t1\n",
      "  (2, 1400)\t1\n",
      "  (3, 1304)\t1\n",
      "  (5, 506)\t1\n",
      "  (5, 567)\t1\n",
      "  (6, 409)\t1\n",
      "  (6, 565)\t1\n",
      "  (6, 737)\t1\n",
      "  (6, 877)\t1\n",
      "  (6, 958)\t1\n",
      "  :\t:\n",
      "  (242, 1152)\t1\n",
      "  (243, 36)\t1\n",
      "  (243, 88)\t1\n",
      "  (243, 180)\t1\n",
      "  (243, 230)\t1\n",
      "  (243, 521)\t1\n",
      "  (243, 764)\t1\n",
      "  (243, 860)\t1\n",
      "  (243, 1214)\t1\n",
      "  (244, 972)\t1\n",
      "  (245, 553)\t1\n",
      "  (245, 813)\t1\n",
      "  (245, 1246)\t1\n",
      "  (246, 121)\t1\n",
      "  (246, 1214)\t1\n",
      "  (246, 1445)\t1\n",
      "  (247, 703)\t1\n",
      "  (247, 737)\t1\n",
      "  (247, 985)\t1\n",
      "  (247, 1293)\t1\n",
      "  (247, 1310)\t1\n",
      "  (248, 553)\t1\n",
      "  (248, 1310)\t1\n",
      "  (249, 1189)\t1\n",
      "  (249, 1311)\t1\n"
     ]
    }
   ],
   "source": [
    "print(test_vector_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿¿¿¿ n-gramas ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    \n",
    "#### Para cada una de esas 4 combinaciones entrenaremos dos clasificadores:\n",
    "\n",
    "     1. Un clasificador naive bayes, eligiendo el más adecuado para cada caso.\n",
    "     \n",
    "     2. Un árbol de decisión buscando un valor óptimo para uno de los siguientes parámetros para que se maximice la tasa de aciertos en el conjunto de test: max_depth, min_samples_leaf o max_leaf_nodes (siempre el mismo).\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento: 0.968\n",
      "Multinomial Naive Bayes, porcentaje de aciertos en test: 0.744\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "\n",
    "mnb_classifier.fit(train_preprocessed, y_train)\n",
    "\n",
    "mnb_train_predictions = mnb_classifier.predict(train_preprocessed)\n",
    "mnb_test_predictions = mnb_classifier.predict(test_preprocessed)\n",
    "\n",
    "print(\"Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento:\", np.mean(mnb_train_predictions == y_train))\n",
    "print(\"Multinomial Naive Bayes, porcentaje de aciertos en test:\", np.mean(mnb_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Árbol, porcentaje de aciertos en entrenamiento: 0.9973333333333333\n",
      "Árbol, porcentaje de aciertos en test: 0.676\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "import numpy as np\n",
    "\n",
    "# Creamos el clasificador con los valores por defecto\n",
    "tree_classifier = tree.DecisionTreeClassifier()\n",
    "tree_classifier.fit(train_preprocessed, y_train)\n",
    "\n",
    "tree_train_predictions = tree_classifier.predict(train_preprocessed)\n",
    "tree_test_predictions = tree_classifier.predict(test_preprocessed)\n",
    "\n",
    "print(\"Árbol, porcentaje de aciertos en entrenamiento:\", np.mean(tree_train_predictions == y_train))\n",
    "print(\"Árbol, porcentaje de aciertos en test:\", np.mean(tree_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analiza la tasa de aciertos de entrenamiento y test de los 2 clasificadores en las 4 representaciones de bolsa de palabras (8 configuraciones en total) y contesta a las siguientes preguntas:\n",
    "\n",
    "    - ¿Hay un clasificador que sea superior al otro? ¿por qué crees que sucede?\n",
    "    \n",
    "    - Para cada clasificador, ¿tiene un efecto positivo el añadir “complejidad” a la vectorización? Es decir, añadir bigramas y añadir tf-idf. ¿Por qué crees que sucede este efecto positivo o la falta del mismo? \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados que obtenemos con el arbol de decisión son mejores que los obtenidos con el clasificador de naive bayes. MIRAR ARBOLES DE DECISION Y NAIVE BAYES (para contestar por que creemos que sucede eso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecciona el mejor árbol de decisión y obtén las 25 variables con más poder discriminante:\n",
    "    \n",
    "    - ¿Predominan más las palabras de uno u otro sentimiento? ¿por qué? ¿hay ruido? \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.78      0.68      0.72       124\n",
      "        Good       0.72      0.81      0.76       126\n",
      "\n",
      "    accuracy                           0.74       250\n",
      "   macro avg       0.75      0.74      0.74       250\n",
      "weighted avg       0.75      0.74      0.74       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# NOTA: Aquí elegimos analizar un determinado clasificador y sus predicciones \n",
    "# Por ejemplo el naive bayes\n",
    "classifier=mnb_classifier\n",
    "predictions = mnb_test_predictions\n",
    "target_names={'Bad','Good'}\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top20_features_in_trees(vectorizer, clf):\n",
    "    \"\"\"Prints features with the highest coefficient values\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    top20 = np.argsort(clf.feature_importances_)[-20:]\n",
    "    reversed_top = top20[::-1]\n",
    "    print(\"Top 20 features in the tree\\n\")\n",
    "    print(\"%s\" % ( \" / \".join(feature_names[j] for j in reversed_top)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 features in the tree\n",
      "\n",
      "great / good / amazing / delicious / best / nice / fantastic / awesome / loved / excellent / perfect / friendly / love / happy / enjoyed / wonderful / come / order / fresh / priced\n"
     ]
    }
   ],
   "source": [
    "print_top20_features_in_trees(vectorizer,tree_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top20_features_per_class_in_NB(vectorizer, clf, class_labels):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    print(\"Top 25 features per class\\n\")\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top20 = np.argsort(clf.feature_log_prob_[i])[-25:]\n",
    "        reversed_top = top20[::-1]\n",
    "        \n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \" / \".join(feature_names[j] for j in reversed_top)),'\\n')\n",
    "        \n",
    "        #Descomentar para ver el índice de los términos en el diccionario\n",
    "        #print(\"%s \" % (\" / \".join(str(j) for j in reversed_top)),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 features per class\n",
      "\n",
      "Bad: food / service / place / won / don / good / like / worst / time / bad / ve / terrible / minutes / disappointed / probably / think / wasn / got / came / just / going / waited / eating / slow / stars \n",
      "\n",
      "Good: good / great / food / place / service / delicious / amazing / really / friendly / love / awesome / time / nice / best / fantastic / just / loved / staff / restaurant / experience / like / definitely / atmosphere / happy / excellent \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top20_features_per_class_in_NB(vectorizer,mnb_classifier,target_names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecciona el mejor clasificador naive bayes y obtén las 25 variables con más presencia en cada clase:\n",
    "    \n",
    "    - ¿Tienen sentido las palabras seleccionadas? ¿hay ruido (palabras sin sentimiento o de sentimiento opuesto al esperado)? ¿por qué crees que suceden estos fenómenos?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalmente, explica de manera razonada las conclusiones que has extraído de todo el estudio realizado en este apartado.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apartado b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toma el mejor clasificador Naive Bayes y el mejor árbol de decisión y analiza a fondo sus resultados en el conjunto de test.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Analiza la precisión y la exhaustividad de cada clasificador en cada una de las clases (opiniones positivas y negativas).\n",
    "    \n",
    "         Para cada clasificador, ¿tiene un comportamiento homogéneo a la hora de clasificar ambas clases?\n",
    "         ¿Cuáles son las fortalezas y debilidades de cada uno de los clasificadores?\n",
    "         ¿Hay algún clasificador que sea mejor que el otro en todo?\n",
    "         ¿Coinciden ambos clasificadores a la hora de clasificar mejor una clase que la otra?\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Pinta los 8 primeros niveles del árbol de decisión y comenta lo que ves.\n",
    "         ¿Qué estructura tiene el árbol?\n",
    "         ¿Cómo interpretas los niveles que has pintado? ¿tienen algún sentido con respecto a la tasa de aciertos, o la precisión y exhaustividad del clasificador? o ¿Hay nodos impuros?\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Por cada clasificador identifica 2 críticas que hayan sido falsas positivas (malas críticas calificadas como buenas) y 2 críticas que han sido falsas negativas (buenas críticas clasificadas como malas). Analiza tanto su texto original, como el vector de palabras resultante (solamente los términos activos).\n",
    "         ¿Por qué crees que ha fallado el clasificador en cada uno de los casos?\n",
    "         ¿Se te ocurre alguna idea sobre cómo mejorar el clasificador de sentimiento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
